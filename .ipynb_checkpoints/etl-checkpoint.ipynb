{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Processes\n",
    "Use this notebook to develop the ETL process for each of your tables before completing the `etl.py` file to load the whole datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pandas as pd\n",
    "from sql_queries import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to create list of all our raw JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(filepath):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.json'))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process `song_data`\n",
    "In this first part, you'll perform ETL on the first dataset, `song_data`, to create the `songs` and `artists` dimensional tables.\n",
    "\n",
    "Let's perform ETL on a single song file and load a single record into each table to start.\n",
    "- Use the `get_files` function provided above to get a list of all song JSON files in `data/song_data`\n",
    "- Select the first song in this list\n",
    "- Read the song file and view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/song_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericr\\Google Drive\\Online Courses\\Udacity\\Data Engineering\\Part 1 - Data Modeling\\Project - Data Modeling with Postgres\\data\\song_data\\A\\A\\A\\TRAAABD128F429CF47.json\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "song_files = get_files(filepath)\n",
    "print(song_files[1])\n",
    "print(len(song_files))\n",
    "## There are 71 song files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 71 song files. Loop through all files and insert into the same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_songs</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_latitude</th>\n",
       "      <th>artist_longitude</th>\n",
       "      <th>artist_location</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>song_id</th>\n",
       "      <th>title</th>\n",
       "      <th>duration</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ARD7TVE1187B99BFB1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>California - LA</td>\n",
       "      <td>Casual</td>\n",
       "      <td>SOMZWCG12A8C13C480</td>\n",
       "      <td>I Didn't Mean To</td>\n",
       "      <td>218.93179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ARMJAGH1187FB546F3</td>\n",
       "      <td>35.14968</td>\n",
       "      <td>-90.04892</td>\n",
       "      <td>Memphis, TN</td>\n",
       "      <td>The Box Tops</td>\n",
       "      <td>SOCIWDW12A8C13D406</td>\n",
       "      <td>Soul Deep</td>\n",
       "      <td>148.03546</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ARKRRTF1187B9984DA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Sonora Santanera</td>\n",
       "      <td>SOXVLOJ12AB0189215</td>\n",
       "      <td>Amor De Cabaret</td>\n",
       "      <td>177.47546</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AR7G5I41187FB4CE6C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London, England</td>\n",
       "      <td>Adam Ant</td>\n",
       "      <td>SONHOTT12A8C13493C</td>\n",
       "      <td>Something Girls</td>\n",
       "      <td>233.40363</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ARXR32B1187FB57099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Gob</td>\n",
       "      <td>SOFSOCN12A8C143F5D</td>\n",
       "      <td>Face the Ashes</td>\n",
       "      <td>209.60608</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AR8IEZO1187B99055E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Marc Shaiman</td>\n",
       "      <td>SOINLJW12A8C13314C</td>\n",
       "      <td>City Slickers</td>\n",
       "      <td>149.86404</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AR558FS1187FB45658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>40 Grit</td>\n",
       "      <td>SOGDBUF12A8C140FAA</td>\n",
       "      <td>Intro</td>\n",
       "      <td>75.67628</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ARVBRGZ1187FB4675A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Gwen Stefani</td>\n",
       "      <td>SORRZGD12A6310DBC3</td>\n",
       "      <td>Harajuku Girls</td>\n",
       "      <td>290.55955</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ARWB3G61187FB49404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hamilton, Ohio</td>\n",
       "      <td>Steve Morse</td>\n",
       "      <td>SODAUVL12A8C13D184</td>\n",
       "      <td>Prognosis</td>\n",
       "      <td>363.85914</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AREVWGE1187B9B890A</td>\n",
       "      <td>-13.44200</td>\n",
       "      <td>-41.99520</td>\n",
       "      <td>Noci (BA)</td>\n",
       "      <td>Bitter End</td>\n",
       "      <td>SOFCHDR12AB01866EF</td>\n",
       "      <td>Living Hell</td>\n",
       "      <td>282.43546</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_songs           artist_id  artist_latitude  artist_longitude  \\\n",
       "0           1  ARD7TVE1187B99BFB1              NaN               NaN   \n",
       "0           1  ARMJAGH1187FB546F3         35.14968         -90.04892   \n",
       "0           1  ARKRRTF1187B9984DA              NaN               NaN   \n",
       "0           1  AR7G5I41187FB4CE6C              NaN               NaN   \n",
       "0           1  ARXR32B1187FB57099              NaN               NaN   \n",
       "..        ...                 ...              ...               ...   \n",
       "0           1  AR8IEZO1187B99055E              NaN               NaN   \n",
       "0           1  AR558FS1187FB45658              NaN               NaN   \n",
       "0           1  ARVBRGZ1187FB4675A              NaN               NaN   \n",
       "0           1  ARWB3G61187FB49404              NaN               NaN   \n",
       "0           1  AREVWGE1187B9B890A        -13.44200         -41.99520   \n",
       "\n",
       "    artist_location       artist_name             song_id             title  \\\n",
       "0   California - LA            Casual  SOMZWCG12A8C13C480  I Didn't Mean To   \n",
       "0       Memphis, TN      The Box Tops  SOCIWDW12A8C13D406         Soul Deep   \n",
       "0                    Sonora Santanera  SOXVLOJ12AB0189215   Amor De Cabaret   \n",
       "0   London, England          Adam Ant  SONHOTT12A8C13493C   Something Girls   \n",
       "0                                 Gob  SOFSOCN12A8C143F5D    Face the Ashes   \n",
       "..              ...               ...                 ...               ...   \n",
       "0                        Marc Shaiman  SOINLJW12A8C13314C     City Slickers   \n",
       "0                             40 Grit  SOGDBUF12A8C140FAA             Intro   \n",
       "0                        Gwen Stefani  SORRZGD12A6310DBC3    Harajuku Girls   \n",
       "0    Hamilton, Ohio       Steve Morse  SODAUVL12A8C13D184         Prognosis   \n",
       "0         Noci (BA)        Bitter End  SOFCHDR12AB01866EF       Living Hell   \n",
       "\n",
       "     duration  year  \n",
       "0   218.93179     0  \n",
       "0   148.03546  1969  \n",
       "0   177.47546     0  \n",
       "0   233.40363  1982  \n",
       "0   209.60608  2007  \n",
       "..        ...   ...  \n",
       "0   149.86404  2008  \n",
       "0    75.67628  2003  \n",
       "0   290.55955  2004  \n",
       "0   363.85914  2000  \n",
       "0   282.43546     0  \n",
       "\n",
       "[71 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_json(song_files[1], lines=True) \n",
    "\n",
    "# Update: Read ALL files into the same dataframe\n",
    "df = pd.DataFrame()\n",
    "for i in range(0, len(song_files)):\n",
    "    temp = pd.read_json(song_files[i], lines=True)\n",
    "    df = df.append(temp)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1: `songs` Table\n",
    "#### Extract Data for Songs Table\n",
    "- Select columns for song ID, title, artist ID, year, and duration\n",
    "- Use `df.values` to select just the values from the dataframe\n",
    "- Index to select the first (only) record in the dataframe\n",
    "- Convert the array to a list and set it to `song_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>year</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOFFKZS12AB017F194</td>\n",
       "      <td>A Higher Place (Album Version)</td>\n",
       "      <td>ARBEBBY1187B9B43DB</td>\n",
       "      <td>1994</td>\n",
       "      <td>236.17261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SODREIN12A58A7F2E5</td>\n",
       "      <td>A Whiter Shade Of Pale (Live @ Fillmore West)</td>\n",
       "      <td>ARLTWXK1187FB5A3F8</td>\n",
       "      <td>0</td>\n",
       "      <td>326.00771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOXVLOJ12AB0189215</td>\n",
       "      <td>Amor De Cabaret</td>\n",
       "      <td>ARKRRTF1187B9984DA</td>\n",
       "      <td>0</td>\n",
       "      <td>177.47546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SORAMLE12AB017C8B0</td>\n",
       "      <td>Auguri Cha Cha</td>\n",
       "      <td>ARHHO3O1187B989413</td>\n",
       "      <td>0</td>\n",
       "      <td>191.84281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOZHPGD12A8C1394FE</td>\n",
       "      <td>Baby Come To Me</td>\n",
       "      <td>AR9AWNF1187B9AB0B4</td>\n",
       "      <td>0</td>\n",
       "      <td>236.93016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOYTPEP12AB0180E7B</td>\n",
       "      <td>Twist and Shout</td>\n",
       "      <td>ARAJPHH1187FB5566A</td>\n",
       "      <td>1964</td>\n",
       "      <td>164.80608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SODUJBS12A8C132150</td>\n",
       "      <td>Wessex Loses a Bride</td>\n",
       "      <td>ARI2JSK1187FB496EF</td>\n",
       "      <td>0</td>\n",
       "      <td>111.62077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOBKWDJ12A8C13B2F3</td>\n",
       "      <td>Wild Rose (Back 2 Basics Mix)</td>\n",
       "      <td>AR36F9J1187FB406F1</td>\n",
       "      <td>0</td>\n",
       "      <td>230.71302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOQHXMF12AB0182363</td>\n",
       "      <td>Young Boy Blues</td>\n",
       "      <td>ARGSJW91187B9B1D6B</td>\n",
       "      <td>0</td>\n",
       "      <td>218.77506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOGOSOV12AF72A285E</td>\n",
       "      <td>¿Dónde va Chichi?</td>\n",
       "      <td>ARGUVEV1187B98BA17</td>\n",
       "      <td>1997</td>\n",
       "      <td>313.12934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               song_id                                          title  \\\n",
       "0   SOFFKZS12AB017F194                 A Higher Place (Album Version)   \n",
       "0   SODREIN12A58A7F2E5  A Whiter Shade Of Pale (Live @ Fillmore West)   \n",
       "0   SOXVLOJ12AB0189215                                Amor De Cabaret   \n",
       "0   SORAMLE12AB017C8B0                                 Auguri Cha Cha   \n",
       "0   SOZHPGD12A8C1394FE                                Baby Come To Me   \n",
       "..                 ...                                            ...   \n",
       "0   SOYTPEP12AB0180E7B                                Twist and Shout   \n",
       "0   SODUJBS12A8C132150                           Wessex Loses a Bride   \n",
       "0   SOBKWDJ12A8C13B2F3                  Wild Rose (Back 2 Basics Mix)   \n",
       "0   SOQHXMF12AB0182363                                Young Boy Blues   \n",
       "0   SOGOSOV12AF72A285E                              ¿Dónde va Chichi?   \n",
       "\n",
       "             artist_id  year   duration  \n",
       "0   ARBEBBY1187B9B43DB  1994  236.17261  \n",
       "0   ARLTWXK1187FB5A3F8     0  326.00771  \n",
       "0   ARKRRTF1187B9984DA     0  177.47546  \n",
       "0   ARHHO3O1187B989413     0  191.84281  \n",
       "0   AR9AWNF1187B9AB0B4     0  236.93016  \n",
       "..                 ...   ...        ...  \n",
       "0   ARAJPHH1187FB5566A  1964  164.80608  \n",
       "0   ARI2JSK1187FB496EF     0  111.62077  \n",
       "0   AR36F9J1187FB406F1     0  230.71302  \n",
       "0   ARGSJW91187B9B1D6B     0  218.77506  \n",
       "0   ARGUVEV1187B98BA17  1997  313.12934  \n",
       "\n",
       "[71 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# song_data = df[['song_id', 'title', 'artist_id', 'year', 'duration']]  # pandas dataframe\n",
    "# song_data = song_data.values   # values only (is now a numpy array)\n",
    "# song_data = song_data[0, :]  # select first record of song_data (not sure why this is necessary)\n",
    "# song_data = song_data.tolist() # Convert array to list\n",
    "\n",
    "# Create song DF from ALL song files\n",
    "song_df = df[['song_id', 'title', 'artist_id', 'year', 'duration']]\n",
    "\n",
    "# Sort by song_id\n",
    "song_df = song_df.sort_values('title')\n",
    "\n",
    "# Save as CSV so we can use COPY for better data loading performance\n",
    "song_df.to_csv(\"data/song_df.csv\", index=False)\n",
    "\n",
    "song_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset tables and connection before inserting records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_tables.py\n",
    "conn = psycopg2.connect(dbstring)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Record into Song Table\n",
    "Implement the `song_table_insert` query in `sql_queries.py` and run the cell below to insert a record for this song into the `songs` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `songs` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur.execute(song_table_insert, song_data)\n",
    "# conn.commit()\n",
    "\n",
    "# Updated query using COPY\n",
    "song_path = datapath+'/song_df.csv'\n",
    "cur.execute(song_table_insert, (song_path,))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added a record to this table.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, close the db connection, or else `create_tables.py` cannot run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2: `artists` Table\n",
    "#### Extract Data for Artists Table\n",
    "- Select columns for artist ID, name, location, latitude, and longitude\n",
    "- Use `df.values` to select just the values from the dataframe\n",
    "- Index to select the first (only) record in the dataframe\n",
    "- Convert the array to a list and set it to `artist_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique artists in song files: 69\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>artist_location</th>\n",
       "      <th>artist_latitude</th>\n",
       "      <th>artist_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR558FS1187FB45658</td>\n",
       "      <td>40 Grit</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR7G5I41187FB4CE6C</td>\n",
       "      <td>Adam Ant</td>\n",
       "      <td>London, England</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARI3BMM1187FB4255E</td>\n",
       "      <td>Alice Stuart</td>\n",
       "      <td>Washington</td>\n",
       "      <td>38.89910</td>\n",
       "      <td>-77.02900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARL7K851187B99ACD2</td>\n",
       "      <td>Andy Andy</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR3JMC51187B9AE49D</td>\n",
       "      <td>Backstreet Boys</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>28.53823</td>\n",
       "      <td>-81.37739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR10USD1187B99F3F1</td>\n",
       "      <td>Tweeterfriendly Music</td>\n",
       "      <td>Burlington, Ontario, Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARC43071187B990240</td>\n",
       "      <td>Wayne Watson</td>\n",
       "      <td>Wisner, LA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR051KA1187B98B2FF</td>\n",
       "      <td>Wilks</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AROUOZZ1187B9ABE51</td>\n",
       "      <td>Willie Bobo</td>\n",
       "      <td>New York, NY [Spanish Harlem]</td>\n",
       "      <td>40.79195</td>\n",
       "      <td>-73.94512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARNPAGP1241B9C7FD4</td>\n",
       "      <td>lextrical</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist_id            artist_name                artist_location  \\\n",
       "0   AR558FS1187FB45658                40 Grit                                  \n",
       "0   AR7G5I41187FB4CE6C               Adam Ant                London, England   \n",
       "0   ARI3BMM1187FB4255E           Alice Stuart                     Washington   \n",
       "0   ARL7K851187B99ACD2              Andy Andy                                  \n",
       "0   AR3JMC51187B9AE49D        Backstreet Boys                    Orlando, FL   \n",
       "..                 ...                    ...                            ...   \n",
       "0   AR10USD1187B99F3F1  Tweeterfriendly Music    Burlington, Ontario, Canada   \n",
       "0   ARC43071187B990240           Wayne Watson                     Wisner, LA   \n",
       "0   AR051KA1187B98B2FF                  Wilks                                  \n",
       "0   AROUOZZ1187B9ABE51            Willie Bobo  New York, NY [Spanish Harlem]   \n",
       "0   ARNPAGP1241B9C7FD4              lextrical                                  \n",
       "\n",
       "    artist_latitude  artist_longitude  \n",
       "0               NaN               NaN  \n",
       "0               NaN               NaN  \n",
       "0          38.89910         -77.02900  \n",
       "0               NaN               NaN  \n",
       "0          28.53823         -81.37739  \n",
       "..              ...               ...  \n",
       "0               NaN               NaN  \n",
       "0               NaN               NaN  \n",
       "0               NaN               NaN  \n",
       "0          40.79195         -73.94512  \n",
       "0               NaN               NaN  \n",
       "\n",
       "[71 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# artist_data = df[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']]  # pandas df\n",
    "# artist_data = artist_data.values   # values only (is now a numpy array)\n",
    "# artist_data = artist_data[0, :]  # select first record of song_data (not sure why this is necessary)\n",
    "# artist_data = artist_data.tolist() # Convert array to list\n",
    "\n",
    "# Create artist DF from ALL song files\n",
    "artist_df = df[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']]\n",
    "\n",
    "# Sort by artist_id\n",
    "artist_df = artist_df.sort_values('artist_name')\n",
    "\n",
    "# Save as CSV so we can use COPY for better data loading performance\n",
    "artist_df.to_csv(\"data/artist_df.csv\", index=False)\n",
    "\n",
    "print(\"Total unique artists in song files:\", len(artist_df['artist_name'].unique()))\n",
    "artist_df\n",
    "\n",
    "# There are 69 unique artists in the 71 song files.\n",
    "# The artists Casual and Clp have 2 songs each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset tables and connection before inserting records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_tables.py\n",
    "conn = psycopg2.connect(dbstring)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Record into Artist Table\n",
    "Implement the `artist_table_insert` query in `sql_queries.py` and run the cell below to insert a record for this song's artist into the `artists` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `artists` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cur.execute(artist_table_insert, artist_data)\n",
    "#conn.commit()\n",
    "\n",
    "# Updated query using COPY\n",
    "artist_path = datapath+'/artist_df.csv'\n",
    "cur.execute(artist_table_insert, (artist_path,))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added a record to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, close the db connection, or else `create_tables.py` cannot run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process `log_data`\n",
    "In this part, you'll perform ETL on the second dataset, `log_data`, to create the `time` and `users` dimensional tables, as well as the `songplays` fact table.\n",
    "\n",
    "Let's perform ETL on a single log file and load a single record into each table.\n",
    "- Use the `get_files` function provided above to get a list of all log JSON files in `data/log_data`\n",
    "- Select the first log file in this list\n",
    "- Read the log file and view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/log_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericr\\Google Drive\\Online Courses\\Udacity\\Data Engineering\\Part 1 - Data Modeling\\Project - Data Modeling with Postgres\\data\\log_data\\2018\\11\\2018-11-01-events.json\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "log_files = get_files(filepath)\n",
    "print(log_files[0])\n",
    "print(len(log_files))\n",
    "## There are 30 log files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 30 log files. Loop through all files and insert into the same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events: 8056\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Walter</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>Frye</td>\n",
       "      <td>NaN</td>\n",
       "      <td>free</td>\n",
       "      <td>San Francisco-Oakland-Hayward, CA</td>\n",
       "      <td>GET</td>\n",
       "      <td>Home</td>\n",
       "      <td>1.540919e+12</td>\n",
       "      <td>38</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>1541105830796</td>\n",
       "      <td>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>Summers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>GET</td>\n",
       "      <td>Home</td>\n",
       "      <td>1.540345e+12</td>\n",
       "      <td>139</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>1541106106796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Des'ree</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>Summers</td>\n",
       "      <td>246.30812</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.540345e+12</td>\n",
       "      <td>139</td>\n",
       "      <td>You Gotta Be</td>\n",
       "      <td>200</td>\n",
       "      <td>1541106106796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>Summers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>GET</td>\n",
       "      <td>Upgrade</td>\n",
       "      <td>1.540345e+12</td>\n",
       "      <td>139</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>1541106132796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr Oizo</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>Summers</td>\n",
       "      <td>144.03873</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.540345e+12</td>\n",
       "      <td>139</td>\n",
       "      <td>Flat 55</td>\n",
       "      <td>200</td>\n",
       "      <td>1541106352796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist       auth firstName gender  itemInSession lastName     length  \\\n",
       "0     None  Logged In    Walter      M              0     Frye        NaN   \n",
       "1     None  Logged In    Kaylee      F              0  Summers        NaN   \n",
       "2  Des'ree  Logged In    Kaylee      F              1  Summers  246.30812   \n",
       "3     None  Logged In    Kaylee      F              2  Summers        NaN   \n",
       "4  Mr Oizo  Logged In    Kaylee      F              3  Summers  144.03873   \n",
       "\n",
       "  level                           location method      page  registration  \\\n",
       "0  free  San Francisco-Oakland-Hayward, CA    GET      Home  1.540919e+12   \n",
       "1  free        Phoenix-Mesa-Scottsdale, AZ    GET      Home  1.540345e+12   \n",
       "2  free        Phoenix-Mesa-Scottsdale, AZ    PUT  NextSong  1.540345e+12   \n",
       "3  free        Phoenix-Mesa-Scottsdale, AZ    GET   Upgrade  1.540345e+12   \n",
       "4  free        Phoenix-Mesa-Scottsdale, AZ    PUT  NextSong  1.540345e+12   \n",
       "\n",
       "   sessionId          song  status             ts  \\\n",
       "0         38          None     200  1541105830796   \n",
       "1        139          None     200  1541106106796   \n",
       "2        139  You Gotta Be     200  1541106106796   \n",
       "3        139          None     200  1541106132796   \n",
       "4        139       Flat 55     200  1541106352796   \n",
       "\n",
       "                                           userAgent userId  \n",
       "0  \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...     39  \n",
       "1  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...      8  \n",
       "2  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...      8  \n",
       "3  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...      8  \n",
       "4  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...      8  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_json(log_files[1], lines=True) \n",
    "\n",
    "# Update: Read ALL files into the same dataframe\n",
    "df = pd.DataFrame()\n",
    "for i in range(0, len(log_files)):\n",
    "    temp = pd.read_json(log_files[i], lines=True)\n",
    "    df = df.append(temp)\n",
    "    \n",
    "print('Total number of events:', len(df))\n",
    "df.head(5)\n",
    "# 8056 total log events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3: `time` Table\n",
    "#### Extract Data for Time Table\n",
    "- Filter records by `NextSong` action\n",
    "- Convert the `ts` timestamp column to datetime\n",
    "  - Hint: the current timestamp is in milliseconds\n",
    "- Extract the timestamp, hour, day, week of year, month, year, and weekday from the `ts` column and set `time_data` to a list containing these values in order\n",
    "  - Hint: use pandas' [`dt` attribute](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.html) to access easily datetimelike properties.\n",
    "- Specify labels for these columns and set to `column_labels`\n",
    "- Create a dataframe, `time_df,` containing the time data for this file by combining `column_labels` and `time_data` into a dictionary and converting this into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of song events: 6820\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Des'ree</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>Summers</td>\n",
       "      <td>246.30812</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.540345e+12</td>\n",
       "      <td>139</td>\n",
       "      <td>You Gotta Be</td>\n",
       "      <td>200</td>\n",
       "      <td>2018-11-01 21:01:46.796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr Oizo</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>Summers</td>\n",
       "      <td>144.03873</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.540345e+12</td>\n",
       "      <td>139</td>\n",
       "      <td>Flat 55</td>\n",
       "      <td>200</td>\n",
       "      <td>2018-11-01 21:05:52.796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tamba Trio</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>Summers</td>\n",
       "      <td>177.18812</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.540345e+12</td>\n",
       "      <td>139</td>\n",
       "      <td>Quem Quiser Encontrar O Amor</td>\n",
       "      <td>200</td>\n",
       "      <td>2018-11-01 21:08:16.796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Mars Volta</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>Summers</td>\n",
       "      <td>380.42077</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.540345e+12</td>\n",
       "      <td>139</td>\n",
       "      <td>Eriatarka</td>\n",
       "      <td>200</td>\n",
       "      <td>2018-11-01 21:11:13.796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Infected Mushroom</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>6</td>\n",
       "      <td>Summers</td>\n",
       "      <td>440.26730</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.540345e+12</td>\n",
       "      <td>139</td>\n",
       "      <td>Becoming Insane</td>\n",
       "      <td>200</td>\n",
       "      <td>2018-11-01 21:17:33.796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              artist       auth firstName gender  itemInSession lastName  \\\n",
       "2            Des'ree  Logged In    Kaylee      F              1  Summers   \n",
       "4            Mr Oizo  Logged In    Kaylee      F              3  Summers   \n",
       "5         Tamba Trio  Logged In    Kaylee      F              4  Summers   \n",
       "6     The Mars Volta  Logged In    Kaylee      F              5  Summers   \n",
       "7  Infected Mushroom  Logged In    Kaylee      F              6  Summers   \n",
       "\n",
       "      length level                     location method      page  \\\n",
       "2  246.30812  free  Phoenix-Mesa-Scottsdale, AZ    PUT  NextSong   \n",
       "4  144.03873  free  Phoenix-Mesa-Scottsdale, AZ    PUT  NextSong   \n",
       "5  177.18812  free  Phoenix-Mesa-Scottsdale, AZ    PUT  NextSong   \n",
       "6  380.42077  free  Phoenix-Mesa-Scottsdale, AZ    PUT  NextSong   \n",
       "7  440.26730  free  Phoenix-Mesa-Scottsdale, AZ    PUT  NextSong   \n",
       "\n",
       "   registration  sessionId                          song  status  \\\n",
       "2  1.540345e+12        139                  You Gotta Be     200   \n",
       "4  1.540345e+12        139                       Flat 55     200   \n",
       "5  1.540345e+12        139  Quem Quiser Encontrar O Amor     200   \n",
       "6  1.540345e+12        139                     Eriatarka     200   \n",
       "7  1.540345e+12        139               Becoming Insane     200   \n",
       "\n",
       "                       ts                                          userAgent  \\\n",
       "2 2018-11-01 21:01:46.796  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...   \n",
       "4 2018-11-01 21:05:52.796  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...   \n",
       "5 2018-11-01 21:08:16.796  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...   \n",
       "6 2018-11-01 21:11:13.796  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...   \n",
       "7 2018-11-01 21:17:33.796  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...   \n",
       "\n",
       "  userId  \n",
       "2      8  \n",
       "4      8  \n",
       "5      8  \n",
       "6      8  \n",
       "7      8  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['page'].str.contains(\"NextSong\")] # Filter to only NextSong actions\n",
    "t = pd.to_datetime(df.ts, unit='ms') # convert ts column to datetime\n",
    "df['ts'] = t\n",
    "\n",
    "# Sort log data df by start_time\n",
    "df = df.sort_values('ts')\n",
    "\n",
    "# Save as CSV for temporary QC purposes\n",
    "df.to_csv(\"data/log_df.csv\", index=True)\n",
    "\n",
    "print('Total number of song events:', len(df))\n",
    "df.head(5)\n",
    "# 6820 log events with NextSong (actual song events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     2018-11-01 21:01:46.796\n",
       "4     2018-11-01 21:05:52.796\n",
       "5     2018-11-01 21:08:16.796\n",
       "6     2018-11-01 21:11:13.796\n",
       "7     2018-11-01 21:17:33.796\n",
       "                ...          \n",
       "382   2018-11-30 18:40:05.796\n",
       "383   2018-11-30 18:44:36.796\n",
       "384   2018-11-30 18:47:58.796\n",
       "385   2018-11-30 18:51:24.796\n",
       "387   2018-11-30 19:54:24.796\n",
       "Name: ts, Length: 6820, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.to_datetime(df.ts, unit='ms') # convert ts column to datetime\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = list(zip(t, t.dt.hour, t.dt.day, t.dt.week, t.dt.month, t.dt.year, t.dt.weekday))\n",
    "column_labels = (\"start_time\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\")\n",
    "#print(time_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time    object\n",
      "hour           int32\n",
      "day            int32\n",
      "week           int32\n",
      "month          int32\n",
      "year           int32\n",
      "weekday        int32\n",
      "dtype: object\n",
      "Total unique start times: 6813\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-01 21:01:46.796000</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-01 21:05:52.796000</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-01 21:08:16.796000</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-01 21:11:13.796000</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-11-01 21:17:33.796000</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>2018-11-30 18:40:05.796000</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>2018-11-30 18:44:36.796000</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>2018-11-30 18:47:58.796000</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>2018-11-30 18:51:24.796000</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6819</th>\n",
       "      <td>2018-11-30 19:54:24.796000</td>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6820 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      start_time  hour  day  week  month  year  weekday\n",
       "0     2018-11-01 21:01:46.796000    21    1    44     11  2018        3\n",
       "1     2018-11-01 21:05:52.796000    21    1    44     11  2018        3\n",
       "2     2018-11-01 21:08:16.796000    21    1    44     11  2018        3\n",
       "3     2018-11-01 21:11:13.796000    21    1    44     11  2018        3\n",
       "4     2018-11-01 21:17:33.796000    21    1    44     11  2018        3\n",
       "...                          ...   ...  ...   ...    ...   ...      ...\n",
       "6815  2018-11-30 18:40:05.796000    18   30    48     11  2018        4\n",
       "6816  2018-11-30 18:44:36.796000    18   30    48     11  2018        4\n",
       "6817  2018-11-30 18:47:58.796000    18   30    48     11  2018        4\n",
       "6818  2018-11-30 18:51:24.796000    18   30    48     11  2018        4\n",
       "6819  2018-11-30 19:54:24.796000    19   30    48     11  2018        4\n",
       "\n",
       "[6820 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_df = pd.DataFrame(time_data, columns = column_labels, dtype = int)\n",
    "\n",
    "# Sort by start_time\n",
    "time_df = time_df.sort_values('start_time')\n",
    "\n",
    "# Save as CSV so we can use COPY for better data loading performance\n",
    "time_df.to_csv(\"data/time_df.csv\", index=False)\n",
    "\n",
    "print(time_df.dtypes)\n",
    "print('Total unique start times:',len(time_df['start_time'].unique()))\n",
    "time_df\n",
    "\n",
    "# NOTE: There are 6820 total song events but 6813 with unique start times.\n",
    "# Therefore, there are 7 duplicate start times.\n",
    "# The final time table in PostgreSQL will have the 6813 unique times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset tables and connection before inserting records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_tables.py\n",
    "conn = psycopg2.connect(dbstring)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Records into Time Table\n",
    "Implement the `time_table_insert` query in `sql_queries.py` and run the cell below to insert records for the timestamps in this log file into the `time` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `time` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in time_df.iterrows():\n",
    "#     cur.execute(time_table_insert, list(row))\n",
    "#     conn.commit()\n",
    "    \n",
    "# Updated query using COPY\n",
    "time_path = datapath+'/time_df.csv'\n",
    "cur.execute(time_table_insert, (time_path,))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added records to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, close the db connection, or else `create_tables.py` cannot run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #4: `users` Table\n",
    "#### Extract Data for Users Table\n",
    "- Select columns for user ID, first name, last name, gender and level and set to `user_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "userId        int32\n",
      "firstName    object\n",
      "lastName     object\n",
      "gender       object\n",
      "level        object\n",
      "dtype: object\n",
      "Total unique users: 96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>firstName</th>\n",
       "      <th>lastName</th>\n",
       "      <th>gender</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2</td>\n",
       "      <td>Jizelle</td>\n",
       "      <td>Benjamin</td>\n",
       "      <td>F</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>Jizelle</td>\n",
       "      <td>Benjamin</td>\n",
       "      <td>F</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2</td>\n",
       "      <td>Jizelle</td>\n",
       "      <td>Benjamin</td>\n",
       "      <td>F</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2</td>\n",
       "      <td>Jizelle</td>\n",
       "      <td>Benjamin</td>\n",
       "      <td>F</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>Jizelle</td>\n",
       "      <td>Benjamin</td>\n",
       "      <td>F</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>101</td>\n",
       "      <td>Jayden</td>\n",
       "      <td>Fox</td>\n",
       "      <td>M</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>101</td>\n",
       "      <td>Jayden</td>\n",
       "      <td>Fox</td>\n",
       "      <td>M</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>101</td>\n",
       "      <td>Jayden</td>\n",
       "      <td>Fox</td>\n",
       "      <td>M</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>101</td>\n",
       "      <td>Jayden</td>\n",
       "      <td>Fox</td>\n",
       "      <td>M</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>101</td>\n",
       "      <td>Jayden</td>\n",
       "      <td>Fox</td>\n",
       "      <td>M</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6820 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     userId firstName  lastName gender level\n",
       "73        2   Jizelle  Benjamin      F  free\n",
       "37        2   Jizelle  Benjamin      F  free\n",
       "108       2   Jizelle  Benjamin      F  free\n",
       "74        2   Jizelle  Benjamin      F  free\n",
       "38        2   Jizelle  Benjamin      F  free\n",
       "..      ...       ...       ...    ...   ...\n",
       "252     101    Jayden       Fox      M  free\n",
       "250     101    Jayden       Fox      M  free\n",
       "247     101    Jayden       Fox      M  free\n",
       "72      101    Jayden       Fox      M  free\n",
       "192     101    Jayden       Fox      M  free\n",
       "\n",
       "[6820 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df = df[['userId', 'firstName', 'lastName', 'gender', 'level']]  # pandas df\n",
    "user_df = user_df.astype({\"userId\": int})\n",
    "\n",
    "# Sort by user_id\n",
    "user_df = user_df.sort_values(\"userId\")\n",
    "\n",
    "# Save as CSV so we can use COPY for better data loading performance\n",
    "user_df.to_csv(\"data/user_df.csv\", index=False)\n",
    "\n",
    "print(type(user_df))\n",
    "print(user_df.dtypes)\n",
    "print('Total unique users:',len(user_df['userId'].unique()))\n",
    "user_df\n",
    "\n",
    "# There are 96 unique users that comprise all 6813 song events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset tables and connection before inserting records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_tables.py\n",
    "conn = psycopg2.connect(dbstring)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Records into Users Table\n",
    "Implement the `user_table_insert` query in `sql_queries.py` and run the cell below to insert records for the users in this log file into the `users` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `users` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in user_df.iterrows():\n",
    "#     cur.execute(user_table_insert, row)\n",
    "#     conn.commit()\n",
    "\n",
    "# Updated query using COPY\n",
    "user_path = datapath+'/user_df.csv'\n",
    "cur.execute(user_table_insert, (user_path,))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added records to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, close the db connection, or else `create_tables.py` cannot run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #5: `songplays` Table\n",
    "#### Reset tables and connection before inserting records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_tables.py\n",
    "conn = psycopg2.connect(dbstring)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repopulate the other 4 tables before creating the songplays table\n",
    "cur.execute(song_table_insert, (song_path,))\n",
    "conn.commit()\n",
    "cur.execute(artist_table_insert, (artist_path,))\n",
    "conn.commit()\n",
    "cur.execute(time_table_insert, (time_path,))\n",
    "conn.commit()\n",
    "cur.execute(user_table_insert, (user_path,))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Data and Songplays Table\n",
    "This one is a little more complicated since information from the songs table, artists table, and original log file are all needed for the `songplays` table. Since the log file does not specify an ID for either the song or the artist, you'll need to get the song ID and artist ID by querying the songs and artists tables to find matches based on song title, artist name, and song duration time.\n",
    "- Implement the `song_select` query in `sql_queries.py` to find the song ID and artist ID based on the title, artist name, and duration of a song.\n",
    "- Select the timestamp, user ID, level, song ID, artist ID, session ID, location, and user agent and set to `songplay_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Records into Songplays Table\n",
    "- Implement the `songplay_table_insert` query and run the cell below to insert records for the songplay actions in this log file into the `songplays` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `songplays` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each row of the log files\n",
    "# NOTE: use enumerate as a loop counter for songplay_id\n",
    "# Index is turned off so we get the row number, not the df index\n",
    "for idx, row in enumerate(df.itertuples(index=False), start=1):\n",
    "\n",
    "    # get songid and artistid from song and artist tables\n",
    "    cur.execute(song_select, (row.song, row.artist, row.length))\n",
    "    results = cur.fetchone()\n",
    "    \n",
    "    if results:\n",
    "        songid, artistid = results\n",
    "    else:\n",
    "        songid, artistid = None, None\n",
    "\n",
    "    # insert songplay record\n",
    "    songplay_data = (idx, row.ts, row.userId, row.level, songid, artistid, \\\n",
    "                                 row.sessionId, row.location, row.userAgent)   \n",
    "    cur.execute(songplay_table_insert, songplay_data)    \n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export songplays table as CSV for QC (not included for final Python file)\n",
    "csv_str = datapath+'/songplay_df.csv'\n",
    "cur.execute(\"COPY songplays TO %s DELIMITER ',' CSV HEADER;\", (csv_str,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6820\n",
      "<class 'tuple'>\n",
      "(6820, Timestamp('2018-11-30 19:54:24.796000'), '5', 'free', None, None, 985, 'Detroit-Warren-Dearborn, MI', '\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.77.4 (KHTML, like Gecko) Version/7.0.5 Safari/537.77.4\"')\n"
     ]
    }
   ],
   "source": [
    "print(idx)\n",
    "print(type(songplay_data))\n",
    "print(songplay_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added records to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Connection to Sparkify Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement `etl.py`\n",
    "### Test section to figure out how to implement `etl.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pandas as pd\n",
    "# Disable pandas SettingWithCopyWarning \n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "from sql_queries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_file(cur, filepath):\n",
    "    # read json file\n",
    "    df_temp = pd.read_json(filepath, lines=True)\n",
    "    return df_temp\n",
    "    \n",
    "def insert_song_data(cur, conn, df, csvpath):\n",
    "    # Create song DF from ALL song files\n",
    "    song_df = df[['song_id', 'title', 'artist_id', 'year', 'duration']]\n",
    "    # Sort by song title\n",
    "    song_df = song_df.sort_values('title')\n",
    "\n",
    "    # Create artist DF from ALL song files\n",
    "    artist_df = df[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']]\n",
    "    # Sort by artist_name\n",
    "    artist_df = artist_df.sort_values('artist_name')\n",
    "    \n",
    "    # Create CSVs so we can use COPY for better data loading performance    \n",
    "    song_df.to_csv((csvpath+'/song_df.csv'), index=False)\n",
    "    artist_df.to_csv((csvpath+'/artist_df.csv'), index=False)\n",
    "    \n",
    "    # Insert song data using COPY                            \n",
    "    copy_path = datapath+'/csv_files/song_df.csv'\n",
    "    cur.execute(song_table_insert, (copy_path,))\n",
    "    conn.commit()\n",
    "\n",
    "    # Insert artist data using COPY\n",
    "    copy_path = datapath+'/csv_files/artist_df.csv'\n",
    "    cur.execute(artist_table_insert, (copy_path,))\n",
    "    conn.commit()\n",
    "        \n",
    "def insert_log_data(cur, conn, df, csvpath):\n",
    "    # filter by NextSong action\n",
    "    df = df[df['page'].str.contains(\"NextSong\")]\n",
    "    # convert timestamp column to datetime\n",
    "    t = pd.to_datetime(df.ts, unit='ms') \n",
    "    df['ts'] = t\n",
    "    \n",
    "    # Create time DF from ALL log files\n",
    "    time_data = list(zip(t, t.dt.hour, t.dt.day, t.dt.week, t.dt.month, t.dt.year, t.dt.weekday))\n",
    "    column_labels = (\"start_time\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\")\n",
    "    time_df = pd.DataFrame(time_data, columns = column_labels, dtype = int)\n",
    "    # Sort by start_time\n",
    "    time_df = time_df.sort_values('start_time')\n",
    "\n",
    "    # Create user DF from ALL log files\n",
    "    user_df = df[['userId', 'firstName', 'lastName', 'gender', 'level']]\n",
    "    user_df = user_df.astype({\"userId\": int})\n",
    "    # Sort by user_id\n",
    "    user_df = user_df.sort_values(\"userId\")\n",
    "    \n",
    "    # Create CSVs so we can use COPY for better data loading performance\n",
    "    time_df.to_csv((csvpath+'/time_df.csv'), index=False)\n",
    "    user_df.to_csv((csvpath+'/user_df.csv'), index=False)\n",
    "\n",
    "    # Insert time data using COPY                            \n",
    "    copy_path = datapath+'/csv_files/time_df.csv'\n",
    "    cur.execute(time_table_insert, (copy_path,))\n",
    "    conn.commit() \n",
    "\n",
    "    # Insert user data using COPY                            \n",
    "    copy_path = datapath+'/csv_files/user_df.csv'\n",
    "    cur.execute(user_table_insert, (copy_path,))\n",
    "    conn.commit() \n",
    "        \n",
    "    # INSERT SONGPLAY DATA: Loop through each row of the log files\n",
    "    # NOTE: use enumerate as a loop counter for songplay_id\n",
    "    # Index is False so we get the row number, not the df index\n",
    "    for idx, row in enumerate(df.itertuples(index=False), start=1):\n",
    "\n",
    "        # get songid and artistid from song and artist tables\n",
    "        cur.execute(song_select, (row.song, row.artist, row.length))\n",
    "        results = cur.fetchone()\n",
    "\n",
    "        if results:\n",
    "            songid, artistid = results\n",
    "        else:\n",
    "            songid, artistid = None, None\n",
    "\n",
    "        # insert songplay record\n",
    "        songplay_data = (idx, row.ts, row.userId, row.level, songid, artistid, \\\n",
    "                                     row.sessionId, row.location, row.userAgent)   \n",
    "        cur.execute(songplay_table_insert, songplay_data)    \n",
    "        conn.commit()\n",
    "        \n",
    "def fill_songplay_data(cur, conn, df, csvpath):\n",
    "    # Create a filled songplay table in Postgres with complete artist and song information\n",
    "    # Use artist_name and song_name from the log data instead of the ids from the song data\n",
    "    # This is because there is only ONE row in the songplays table with NON NULL song_id and artist ids   \n",
    "    df = df[df['page'].str.contains(\"NextSong\")] # Filter to only NextSong actions\n",
    "    t = pd.to_datetime(df.ts, unit='ms') # convert ts column to datetime\n",
    "    df['ts'] = t\n",
    "    # Sort by start_time\n",
    "    df = df.sort_values('ts')   \n",
    "    # Select columns (using artist NAME and song NAME instead of ids)\n",
    "    songplay_df = df[['ts', 'userId', 'level', 'song', 'artist', 'sessionId', 'location', 'userAgent']]\n",
    "    # Insert songplay_id column (each row is a different songplay_id)\n",
    "    songplay_df['songplay_id'] = songplay_df.reset_index().index\n",
    "    songplay_df = songplay_df[['songplay_id', 'ts', 'userId', 'level', 'song', 'artist', 'sessionId', 'location', \\\n",
    "                               'userAgent']]\n",
    "    # Create CSV so we can use COPY for better data loading performance\n",
    "    songplay_df.to_csv(csvpath+'/songplay_df.csv', index=False)\n",
    "    # Insert data using COPY                            \n",
    "    copy_path = datapath+'/csv_files/songplay_df.csv'\n",
    "    cur.execute(songplay_table_insert_2, (copy_path,))\n",
    "    conn.commit() \n",
    "    return songplay_df\n",
    "        \n",
    "def quality_check_data(cur, conn, df, idcol, tablename, reset_query):\n",
    "    # Ensure the number of table rows in Postgres equals the number of unique ids \n",
    "    # from the json data. Reset the tables in Postgres if the check fails\n",
    "    table_rows = cur.execute(sql.SQL('SELECT COUNT(*) FROM {}').format(sql.Identifier(tablename)))\n",
    "    table_rows = cur.fetchone()\n",
    "    print('{} total rows in {} table in PostGres.'.format(table_rows[0], tablename))\n",
    "    if tablename == \"songplays\":\n",
    "        num_ids = len(df) # each row is a different songplay_id in the songplays table\n",
    "    else:\n",
    "        num_ids = len(df[idcol].unique())\n",
    "    print('{} total unique {}s from the json files'.format(num_ids, idcol))\n",
    "    if table_rows[0] != num_ids:\n",
    "        cur.execute(sql.SQL('DROP TABLE IF EXISTS {}').format(sql.Identifier(tablename)))\n",
    "        cur.execute(reset_query)\n",
    "        conn.commit()\n",
    "        raise ValueError(\"{} table: Table length and unique ids do not match. Resetting table in PostGres\" \\\n",
    "        .format(tablename))\n",
    "    else:\n",
    "        print(\"Check passed!\")\n",
    "        \n",
    "def process_data(cur, conn, filepath, func):\n",
    "    # get all files matching extension from directory\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.json'))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "\n",
    "    # get total number of files found\n",
    "    num_files = len(all_files)\n",
    "    print('{} files found in {}'.format(num_files, filepath))\n",
    "\n",
    "    # iterate over files and process\n",
    "    df = pd.DataFrame()\n",
    "    for i, datafile in enumerate(all_files, 1):\n",
    "        df_temp = func(cur, datafile)\n",
    "        df = df.append(df_temp)\n",
    "        #conn.commit()\n",
    "        \n",
    "    print('{}/{} total files processed.'.format(i, num_files))\n",
    "    # Return complete df with data from all files    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    conn = psycopg2.connect(dbstring) # dbstring defined at top of sql_queries\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    process_data(cur, conn, filepath='data/song_data', func=process_json_file)\n",
    "    process_data(cur, conn, filepath='data/log_data', func=process_json_file)\n",
    "\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_tables.py\n",
    "conn = psycopg2.connect(dbstring)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 files found in data/song_data\n",
      "71/71 total files processed.\n",
      "71 total rows in songs table in PostGres.\n",
      "71 total unique song_ids from the json files\n",
      "Check passed!\n",
      "69 total rows in artists table in PostGres.\n",
      "69 total unique artist_ids from the json files\n",
      "Check passed!\n"
     ]
    }
   ],
   "source": [
    "# Create df of all song data\n",
    "df = process_data(cur, conn, filepath='data/song_data', func=process_json_file)\n",
    "# Create song and artist dfs and csv files, then copy the data into the Postgres tables\n",
    "insert_song_data(cur, conn, df, csvpath='data/csv_files')\n",
    "\n",
    "# Quality check song and artist tables: Ensure the number of table rows in Postgres\n",
    "# equals the number of unique song and artist ids from the song data\n",
    "quality_check_data(cur, conn, df, idcol=\"song_id\", tablename=\"songs\", reset_query=song_table_create)\n",
    "quality_check_data(cur, conn, df, idcol=\"artist_id\", tablename=\"artists\", reset_query=artist_table_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 files found in data/log_data\n",
      "30/30 total files processed.\n"
     ]
    }
   ],
   "source": [
    "# Create df of all the log data\n",
    "df = process_data(cur, conn, filepath='data/log_data', func=process_json_file)\n",
    "# Create time and user dfs and csv files, then copy into Postgres tables\n",
    "# Then loop through all NextSong events and insert records into the songplay table\n",
    "insert_log_data(cur, conn, df, csvpath='data/csv_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6813 total rows in time table in PostGres.\n",
      "6813 total unique tss from the json files\n",
      "Check passed!\n",
      "96 total rows in users table in PostGres.\n",
      "96 total unique userIds from the json files\n",
      "Check passed!\n",
      "6820 total rows in songplays table in PostGres.\n",
      "6820 total unique songplay_ids from the json files\n",
      "Check passed!\n"
     ]
    }
   ],
   "source": [
    "# Quality check time, user, and songplay tables: Ensure the number of table rows in Postgres\n",
    "# equals the number of unique ids from the log data\n",
    "df = df[df['page'].str.contains(\"NextSong\")] # Filter to only NextSong actions\n",
    "df = df.astype({\"userId\": int}) # Set userId to all integers\n",
    "quality_check_data(cur, conn, df, idcol=\"ts\", tablename = \"time\", reset_query=time_table_create)\n",
    "quality_check_data(cur, conn, df, idcol=\"userId\", tablename = \"users\", reset_query=user_table_create)\n",
    "quality_check_data(cur, conn, df, idcol=\"songplay_id\", tablename = \"songplays\", reset_query=songplay_table_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6820 total rows in songplays_fill table in PostGres.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'songplay_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'songplay_id'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2acfd09979e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Quality check filled songplays table: Ensure the number of table rows in Postgres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# equals the number of unique songplay ids from the log data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mquality_check_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"songplay_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtablename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"songplays_fill\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_query\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msongplay_table_create_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-d29e99c41837>\u001b[0m in \u001b[0;36mquality_check_data\u001b[1;34m(cur, conn, df, idcol, tablename, reset_query)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mnum_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# each row is a different songplay_id in the songplays table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mnum_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{} total unique {}s from the json files'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtable_rows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnum_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'songplay_id'"
     ]
    }
   ],
   "source": [
    "# Create filled songplays df, save as csv, then copy data into Postgres table\n",
    "songplay_df = fill_songplay_data(cur, conn, df, csvpath='data/csv_files')\n",
    "# Quality check filled songplays table: Ensure the number of table rows in Postgres\n",
    "# equals the number of unique songplay ids from the log data\n",
    "quality_check_data(cur, conn, songplay_df, idcol=\"songplay_id\", tablename = \"songplays_fill\", \\\n",
    "                   reset_query=songplay_table_create_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
