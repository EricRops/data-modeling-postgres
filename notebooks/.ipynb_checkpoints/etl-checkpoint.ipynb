{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Processes\n",
    "Use this notebook to develop the ETL process for each of your tables before completing the `etl.py` file to load the whole datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "import glob\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pandas as pd\n",
    "from sql_queries import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to create list of all our raw JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(filepath):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.json'))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process `song_data`\n",
    "In this first part, you'll perform ETL on the first dataset, `song_data`, to create the `songs` and `artists` dimensional tables.\n",
    "\n",
    "Let's perform ETL on a single song file and load a single record into each table to start.\n",
    "- Use the `get_files` function provided above to get a list of all song JSON files in `data/song_data`\n",
    "- Select the first song in this list\n",
    "- Read the song file and view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/song_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_files = get_files(filepath)\n",
    "print(song_files[1])\n",
    "print(len(song_files))\n",
    "## There are 71 song files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 71 song files. Loop through all files and insert into the same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(song_files[1], lines=True) \n",
    "\n",
    "# Update: Read ALL files into the same dataframe\n",
    "df = pd.DataFrame()\n",
    "for i in range(0, len(song_files)):\n",
    "    temp = pd.read_json(song_files[i], lines=True)\n",
    "    df = df.append(temp)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1: `songs` Table\n",
    "#### Extract Data for Songs Table\n",
    "- Select columns for song ID, title, artist ID, year, and duration\n",
    "- Use `df.values` to select just the values from the dataframe\n",
    "- Index to select the first (only) record in the dataframe\n",
    "- Convert the array to a list and set it to `song_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# song_data = df[['song_id', 'title', 'artist_id', 'year', 'duration']]  # pandas dataframe\n",
    "# song_data = song_data.values   # values only (is now a numpy array)\n",
    "# song_data = song_data[0, :]  # select first record of song_data (not sure why this is necessary)\n",
    "# song_data = song_data.tolist() # Convert array to list\n",
    "\n",
    "# Create song DF from ALL song files\n",
    "song_df = df[['song_id', 'title', 'artist_id', 'year', 'duration']]\n",
    "\n",
    "# Sort by song_id\n",
    "song_df = song_df.sort_values('title')\n",
    "\n",
    "# Save as CSV so we can use COPY for better data loading performance\n",
    "song_df.to_csv(\"data/song_df.csv\", index=False)\n",
    "\n",
    "song_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset tables and connection before inserting records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_tables.py\n",
    "conn = psycopg2.connect(dbstring)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Record into Song Table\n",
    "Implement the `song_table_insert` query in `sql_queries.py` and run the cell below to insert a record for this song into the `songs` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `songs` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur.execute(song_table_insert, song_data)\n",
    "# conn.commit()\n",
    "\n",
    "# Updated query using COPY\n",
    "song_path = datapath+'/song_df.csv'\n",
    "cur.execute(song_table_insert, (song_path,))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added a record to this table.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, close the db connection, or else `create_tables.py` cannot run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2: `artists` Table\n",
    "#### Extract Data for Artists Table\n",
    "- Select columns for artist ID, name, location, latitude, and longitude\n",
    "- Use `df.values` to select just the values from the dataframe\n",
    "- Index to select the first (only) record in the dataframe\n",
    "- Convert the array to a list and set it to `artist_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artist_data = df[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']]  # pandas df\n",
    "# artist_data = artist_data.values   # values only (is now a numpy array)\n",
    "# artist_data = artist_data[0, :]  # select first record of song_data (not sure why this is necessary)\n",
    "# artist_data = artist_data.tolist() # Convert array to list\n",
    "\n",
    "# Create artist DF from ALL song files\n",
    "artist_df = df[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']]\n",
    "\n",
    "# Sort by artist_id\n",
    "artist_df = artist_df.sort_values('artist_name')\n",
    "\n",
    "# Save as CSV so we can use COPY for better data loading performance\n",
    "artist_df.to_csv(\"data/artist_df.csv\", index=False)\n",
    "\n",
    "print(\"Total unique artists in song files:\", len(artist_df['artist_name'].unique()))\n",
    "artist_df\n",
    "\n",
    "# There are 69 unique artists in the 71 song files.\n",
    "# The artists Casual and Clp have 2 songs each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset tables and connection before inserting records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_tables.py\n",
    "conn = psycopg2.connect(dbstring)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Record into Artist Table\n",
    "Implement the `artist_table_insert` query in `sql_queries.py` and run the cell below to insert a record for this song's artist into the `artists` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `artists` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cur.execute(artist_table_insert, artist_data)\n",
    "#conn.commit()\n",
    "\n",
    "# Updated query using COPY\n",
    "artist_path = datapath+'/artist_df.csv'\n",
    "cur.execute(artist_table_insert, (artist_path,))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added a record to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, close the db connection, or else `create_tables.py` cannot run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process `log_data`\n",
    "In this part, you'll perform ETL on the second dataset, `log_data`, to create the `time` and `users` dimensional tables, as well as the `songplays` fact table.\n",
    "\n",
    "Let's perform ETL on a single log file and load a single record into each table.\n",
    "- Use the `get_files` function provided above to get a list of all log JSON files in `data/log_data`\n",
    "- Select the first log file in this list\n",
    "- Read the log file and view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/log_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_files = get_files(filepath)\n",
    "print(log_files[0])\n",
    "print(len(log_files))\n",
    "## There are 30 log files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 30 log files. Loop through all files and insert into the same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_json(log_files[1], lines=True) \n",
    "\n",
    "# Update: Read ALL files into the same dataframe\n",
    "df = pd.DataFrame()\n",
    "for i in range(0, len(log_files)):\n",
    "    temp = pd.read_json(log_files[i], lines=True)\n",
    "    df = df.append(temp)\n",
    "    \n",
    "print('Total number of events:', len(df))\n",
    "df.head(5)\n",
    "# 8056 total log events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3: `time` Table\n",
    "#### Extract Data for Time Table\n",
    "- Filter records by `NextSong` action\n",
    "- Convert the `ts` timestamp column to datetime\n",
    "  - Hint: the current timestamp is in milliseconds\n",
    "- Extract the timestamp, hour, day, week of year, month, year, and weekday from the `ts` column and set `time_data` to a list containing these values in order\n",
    "  - Hint: use pandas' [`dt` attribute](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.html) to access easily datetimelike properties.\n",
    "- Specify labels for these columns and set to `column_labels`\n",
    "- Create a dataframe, `time_df,` containing the time data for this file by combining `column_labels` and `time_data` into a dictionary and converting this into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['page'].str.contains(\"NextSong\")] # Filter to only NextSong actions\n",
    "t = pd.to_datetime(df.ts, unit='ms') # convert ts column to datetime\n",
    "df['ts'] = t\n",
    "\n",
    "# Sort log data df by start_time\n",
    "df = df.sort_values('ts')\n",
    "\n",
    "# Save as CSV for temporary QC purposes\n",
    "df.to_csv(\"data/log_df.csv\", index=True)\n",
    "\n",
    "print('Total number of song events:', len(df))\n",
    "df.head(5)\n",
    "# 6820 log events with NextSong (actual song events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.to_datetime(df.ts, unit='ms') # convert ts column to datetime\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = list(zip(t, t.dt.hour, t.dt.day, t.dt.week, t.dt.month, t.dt.year, t.dt.weekday))\n",
    "column_labels = (\"start_time\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\")\n",
    "#print(time_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = pd.DataFrame(time_data, columns = column_labels, dtype = int)\n",
    "\n",
    "# Sort by start_time\n",
    "time_df = time_df.sort_values('start_time')\n",
    "\n",
    "# Save as CSV so we can use COPY for better data loading performance\n",
    "time_df.to_csv(\"data/time_df.csv\", index=False)\n",
    "\n",
    "print(time_df.dtypes)\n",
    "print('Total unique start times:',len(time_df['start_time'].unique()))\n",
    "time_df\n",
    "\n",
    "# NOTE: There are 6820 total song events but 6813 with unique start times.\n",
    "# Therefore, there are 7 duplicate start times.\n",
    "# The final time table in PostgreSQL will have the 6813 unique times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset tables and connection before inserting records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_tables.py\n",
    "conn = psycopg2.connect(dbstring)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Records into Time Table\n",
    "Implement the `time_table_insert` query in `sql_queries.py` and run the cell below to insert records for the timestamps in this log file into the `time` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `time` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in time_df.iterrows():\n",
    "#     cur.execute(time_table_insert, list(row))\n",
    "#     conn.commit()\n",
    "    \n",
    "# Updated query using COPY\n",
    "time_path = datapath+'/time_df.csv'\n",
    "cur.execute(time_table_insert, (time_path,))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added records to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, close the db connection, or else `create_tables.py` cannot run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #4: `users` Table\n",
    "#### Extract Data for Users Table\n",
    "- Select columns for user ID, first name, last name, gender and level and set to `user_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = df[['userId', 'firstName', 'lastName', 'gender', 'level']]  # pandas df\n",
    "user_df = user_df.astype({\"userId\": int})\n",
    "\n",
    "# Sort by user_id\n",
    "user_df = user_df.sort_values(\"userId\")\n",
    "\n",
    "# Save as CSV so we can use COPY for better data loading performance\n",
    "user_df.to_csv(\"data/user_df.csv\", index=False)\n",
    "\n",
    "print(type(user_df))\n",
    "print(user_df.dtypes)\n",
    "print('Total unique users:',len(user_df['userId'].unique()))\n",
    "user_df\n",
    "\n",
    "# There are 96 unique users that comprise all 6813 song events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset tables and connection before inserting records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_tables.py\n",
    "conn = psycopg2.connect(dbstring)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Records into Users Table\n",
    "Implement the `user_table_insert` query in `sql_queries.py` and run the cell below to insert records for the users in this log file into the `users` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `users` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in user_df.iterrows():\n",
    "#     cur.execute(user_table_insert, row)\n",
    "#     conn.commit()\n",
    "\n",
    "# Updated query using COPY\n",
    "user_path = datapath+'/user_df.csv'\n",
    "cur.execute(user_table_insert, (user_path,))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added records to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, close the db connection, or else `create_tables.py` cannot run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #5: `songplays` Table\n",
    "#### Reset tables and connection before inserting records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_tables.py\n",
    "conn = psycopg2.connect(dbstring)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repopulate the other 4 tables before creating the songplays table\n",
    "cur.execute(song_table_insert, (song_path,))\n",
    "conn.commit()\n",
    "cur.execute(artist_table_insert, (artist_path,))\n",
    "conn.commit()\n",
    "cur.execute(time_table_insert, (time_path,))\n",
    "conn.commit()\n",
    "cur.execute(user_table_insert, (user_path,))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Data and Songplays Table\n",
    "This one is a little more complicated since information from the songs table, artists table, and original log file are all needed for the `songplays` table. Since the log file does not specify an ID for either the song or the artist, you'll need to get the song ID and artist ID by querying the songs and artists tables to find matches based on song title, artist name, and song duration time.\n",
    "- Implement the `song_select` query in `sql_queries.py` to find the song ID and artist ID based on the title, artist name, and duration of a song.\n",
    "- Select the timestamp, user ID, level, song ID, artist ID, session ID, location, and user agent and set to `songplay_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Records into Songplays Table\n",
    "- Implement the `songplay_table_insert` query and run the cell below to insert records for the songplay actions in this log file into the `songplays` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `songplays` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each row of the log files\n",
    "# NOTE: use enumerate as a loop counter for songplay_id\n",
    "# Index is turned off so we get the row number, not the df index\n",
    "for idx, row in enumerate(df.itertuples(index=False), start=1):\n",
    "\n",
    "    # get songid and artistid from song and artist tables\n",
    "    cur.execute(song_select, (row.song, row.artist, row.length))\n",
    "    results = cur.fetchone()\n",
    "    \n",
    "    if results:\n",
    "        songid, artistid = results\n",
    "    else:\n",
    "        songid, artistid = None, None\n",
    "\n",
    "    # insert songplay record\n",
    "    songplay_data = (idx, row.ts, row.userId, row.level, songid, artistid, \\\n",
    "                                 row.sessionId, row.location, row.userAgent)   \n",
    "    cur.execute(songplay_table_insert, songplay_data)    \n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export songplays table as CSV for QC (not included for final Python file)\n",
    "csv_str = datapath+'/songplay_df.csv'\n",
    "cur.execute(\"COPY songplays TO %s DELIMITER ',' CSV HEADER;\", (csv_str,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx)\n",
    "print(type(songplay_data))\n",
    "print(songplay_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added records to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Connection to Sparkify Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement `etl.py`\n",
    "### Test section to figure out how to implement `etl.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pandas as pd\n",
    "# Disable pandas SettingWithCopyWarning \n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "from sql_queries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_file(cur, filepath):\n",
    "    # read json file\n",
    "    df_temp = pd.read_json(filepath, lines=True)\n",
    "    return df_temp\n",
    "    \n",
    "def insert_song_data(cur, conn, df, csvpath):\n",
    "    # Create song DF from ALL song files\n",
    "    song_df = df[['song_id', 'title', 'artist_id', 'year', 'duration']]\n",
    "    # Sort by song title\n",
    "    song_df = song_df.sort_values('title')\n",
    "\n",
    "    # Create artist DF from ALL song files\n",
    "    artist_df = df[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']]\n",
    "    # Sort by artist_name\n",
    "    artist_df = artist_df.sort_values('artist_name')\n",
    "    \n",
    "    # Create CSVs so we can use COPY for better data loading performance    \n",
    "    song_df.to_csv((csvpath+'/song_df.csv'), index=False)\n",
    "    artist_df.to_csv((csvpath+'/artist_df.csv'), index=False)\n",
    "    \n",
    "    # Insert song data using COPY                            \n",
    "    copy_path = datapath+'/csv_files/song_df.csv'\n",
    "    cur.execute(song_table_insert, (copy_path,))\n",
    "    conn.commit()\n",
    "\n",
    "    # Insert artist data using COPY\n",
    "    copy_path = datapath+'/csv_files/artist_df.csv'\n",
    "    cur.execute(artist_table_insert, (copy_path,))\n",
    "    conn.commit()\n",
    "        \n",
    "def insert_log_data(cur, conn, df, csvpath):\n",
    "    # filter by NextSong action\n",
    "    df = df[df['page'].str.contains(\"NextSong\")]\n",
    "    # convert timestamp column to datetime\n",
    "    t = pd.to_datetime(df.ts, unit='ms') \n",
    "    df['ts'] = t\n",
    "    \n",
    "    # Create time DF from ALL log files\n",
    "    time_data = list(zip(t, t.dt.hour, t.dt.day, t.dt.week, t.dt.month, t.dt.year, t.dt.weekday))\n",
    "    column_labels = (\"start_time\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\")\n",
    "    time_df = pd.DataFrame(time_data, columns = column_labels, dtype = int)\n",
    "    # Sort by start_time\n",
    "    time_df = time_df.sort_values('start_time')\n",
    "\n",
    "    # Create user DF from ALL log files\n",
    "    user_df = df[['userId', 'firstName', 'lastName', 'gender', 'level']]\n",
    "    user_df = user_df.astype({\"userId\": int})\n",
    "    # Sort by user_id\n",
    "    user_df = user_df.sort_values(\"userId\")\n",
    "    \n",
    "    # Create CSVs so we can use COPY for better data loading performance\n",
    "    time_df.to_csv((csvpath+'/time_df.csv'), index=False)\n",
    "    user_df.to_csv((csvpath+'/user_df.csv'), index=False)\n",
    "\n",
    "    # Insert time data using COPY                            \n",
    "    copy_path = datapath+'/csv_files/time_df.csv'\n",
    "    cur.execute(time_table_insert, (copy_path,))\n",
    "    conn.commit() \n",
    "\n",
    "    # Insert user data using COPY                            \n",
    "    copy_path = datapath+'/csv_files/user_df.csv'\n",
    "    cur.execute(user_table_insert, (copy_path,))\n",
    "    conn.commit() \n",
    "        \n",
    "    # INSERT SONGPLAY DATA: Loop through each row of the log files\n",
    "    # NOTE: use enumerate as a loop counter for songplay_id\n",
    "    # Index is False so we get the row number, not the df index\n",
    "    for idx, row in enumerate(df.itertuples(index=False), start=1):\n",
    "\n",
    "        # get songid and artistid from song and artist tables\n",
    "        cur.execute(song_select, (row.song, row.artist, row.length))\n",
    "        results = cur.fetchone()\n",
    "\n",
    "        if results:\n",
    "            songid, artistid = results\n",
    "        else:\n",
    "            songid, artistid = None, None\n",
    "\n",
    "        # insert songplay record\n",
    "        songplay_data = (idx, row.ts, row.userId, row.level, songid, artistid, \\\n",
    "                                     row.sessionId, row.location, row.userAgent)   \n",
    "        cur.execute(songplay_table_insert, songplay_data)    \n",
    "        conn.commit()\n",
    "        \n",
    "def fill_songplay_data(cur, conn, df, csvpath):\n",
    "    # Create a filled songplay table in Postgres with complete artist and song information\n",
    "    # Use artist_name and song_name from the log data instead of the ids from the song data\n",
    "    # This is because there is only ONE row in the songplays table with NON NULL song_id and artist ids   \n",
    "    df = df[df['page'].str.contains(\"NextSong\")] # Filter to only NextSong actions\n",
    "    t = pd.to_datetime(df.ts, unit='ms') # convert ts column to datetime\n",
    "    df['ts'] = t\n",
    "    # Sort by start_time\n",
    "    df = df.sort_values('ts')   \n",
    "    # Select columns (using artist NAME and song NAME instead of ids)\n",
    "    songplay_df = df[['ts', 'userId', 'level', 'song', 'artist', 'sessionId', 'location', 'userAgent']]\n",
    "    # Insert songplay_id column (each row is a different songplay_id)\n",
    "    songplay_df['songplay_id'] = songplay_df.reset_index().index\n",
    "    songplay_df = songplay_df[['songplay_id', 'ts', 'userId', 'level', 'song', 'artist', 'sessionId', 'location', \\\n",
    "                               'userAgent']]\n",
    "    # Create CSV so we can use COPY for better data loading performance\n",
    "    songplay_df.to_csv(csvpath+'/songplay_df.csv', index=False)\n",
    "    # Insert data using COPY                            \n",
    "    copy_path = datapath+'/csv_files/songplay_df.csv'\n",
    "    cur.execute(songplay_table_insert_2, (copy_path,))\n",
    "    conn.commit() \n",
    "    return songplay_df\n",
    "        \n",
    "def quality_check_data(cur, conn, df, idcol, tablename, reset_query):\n",
    "    # Ensure the number of table rows in Postgres equals the number of unique ids \n",
    "    # from the json data. Reset the tables in Postgres if the check fails\n",
    "    table_rows = cur.execute(sql.SQL('SELECT COUNT(*) FROM {}').format(sql.Identifier(tablename)))\n",
    "    table_rows = cur.fetchone()\n",
    "    print('{} total rows in {} table in PostGres.'.format(table_rows[0], tablename))\n",
    "    if tablename == \"songplays\":\n",
    "        num_ids = len(df) # each row is a different songplay_id in the songplays table\n",
    "    else:\n",
    "        num_ids = len(df[idcol].unique())\n",
    "    print('{} total unique {}s from the json files'.format(num_ids, idcol))\n",
    "    if table_rows[0] != num_ids:\n",
    "        cur.execute(sql.SQL('DROP TABLE IF EXISTS {}').format(sql.Identifier(tablename)))\n",
    "        cur.execute(reset_query)\n",
    "        conn.commit()\n",
    "        raise ValueError(\"{} table: Table length and unique ids do not match. Resetting table in PostGres\" \\\n",
    "        .format(tablename))\n",
    "    else:\n",
    "        print(\"Check passed!\")\n",
    "        \n",
    "def process_data(cur, conn, filepath, func):\n",
    "    # get all files matching extension from directory\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.json'))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "\n",
    "    # get total number of files found\n",
    "    num_files = len(all_files)\n",
    "    print('{} files found in {}'.format(num_files, filepath))\n",
    "\n",
    "    # iterate over files and process\n",
    "    df = pd.DataFrame()\n",
    "    for i, datafile in enumerate(all_files, 1):\n",
    "        df_temp = func(cur, datafile)\n",
    "        df = df.append(df_temp)\n",
    "        #conn.commit()\n",
    "        \n",
    "    print('{}/{} total files processed.'.format(i, num_files))\n",
    "    # Return complete df with data from all files    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    conn = psycopg2.connect(dbstring) # dbstring defined at top of sql_queries\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    process_data(cur, conn, filepath='data/song_data', func=process_json_file)\n",
    "    process_data(cur, conn, filepath='data/log_data', func=process_json_file)\n",
    "\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_tables.py\n",
    "conn = psycopg2.connect(dbstring)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df of all song data\n",
    "df = process_data(cur, conn, filepath='data/song_data', func=process_json_file)\n",
    "# Create song and artist dfs and csv files, then copy the data into the Postgres tables\n",
    "insert_song_data(cur, conn, df, csvpath='data/csv_files')\n",
    "\n",
    "# Quality check song and artist tables: Ensure the number of table rows in Postgres\n",
    "# equals the number of unique song and artist ids from the song data\n",
    "quality_check_data(cur, conn, df, idcol=\"song_id\", tablename=\"songs\", reset_query=song_table_create)\n",
    "quality_check_data(cur, conn, df, idcol=\"artist_id\", tablename=\"artists\", reset_query=artist_table_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df of all the log data\n",
    "df = process_data(cur, conn, filepath='data/log_data', func=process_json_file)\n",
    "# Create time and user dfs and csv files, then copy into Postgres tables\n",
    "# Then loop through all NextSong events and insert records into the songplay table\n",
    "insert_log_data(cur, conn, df, csvpath='data/csv_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality check time, user, and songplay tables: Ensure the number of table rows in Postgres\n",
    "# equals the number of unique ids from the log data\n",
    "df = df[df['page'].str.contains(\"NextSong\")] # Filter to only NextSong actions\n",
    "df = df.astype({\"userId\": int}) # Set userId to all integers\n",
    "quality_check_data(cur, conn, df, idcol=\"ts\", tablename = \"time\", reset_query=time_table_create)\n",
    "quality_check_data(cur, conn, df, idcol=\"userId\", tablename = \"users\", reset_query=user_table_create)\n",
    "quality_check_data(cur, conn, df, idcol=\"songplay_id\", tablename = \"songplays\", reset_query=songplay_table_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filled songplay table in Postgres with complete artist and song information\n",
    "# Use artist_name and song_name from the log data instead of the ids from the song data\n",
    "# This is because there is only ONE row in the songplays table with NON NULL song_id and artist ids   \n",
    "songplay_df = fill_songplay_data(cur, conn, df, csvpath='data/csv_files')\n",
    "# Quality check filled songplays table: Ensure the number of table rows in Postgres\n",
    "# equals the number of unique songplay ids from the log data\n",
    "quality_check_data(cur, conn, songplay_df, idcol=\"songplay_id\", tablename = \"songplays_fill\", \\\n",
    "                   reset_query=songplay_table_create_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
